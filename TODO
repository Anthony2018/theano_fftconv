TODO

- with the new scikits-cuda, the rescaling should be fast. Switch to rescaling inside the IFFT again (and profile)

- mult_and_reduce_batched_complex_dot is not using specified shape info yet. Fix this.

- make a batched complex dot product op using scikits.cuda's lowlevel api with cublasCgemm and multiple streams for parallellisation

- figure out how we can use cublasCgemmBatched for this instead (it's not in scikits.cuda)
    * will need to upgrade cuda for this, it's not in cuda 5.0 (but probably in 5.5 and 6.0). Wait for now.








* look into using multiple CUDA streams to parallellise the computation of the dot products, this should be possible from scikits.cuda/PyCUDA.

* there is also cublasSgemmBatched, and cublasCgemmBatched - only the former is in scikits.cuda though. But this seems to do precisely what we need.





- use multiple cuda streams to do all the dot products concurrently? See "batching kernels": http://docs.nvidia.com/cuda/cublas/index.html#batching-kernels

- somewhere in the code, it seems that 6-dim tensors are still being multiplied, even though this no longer should be happening. Figure out what's going on. Possibly Theano is being overzealous optimizing the Scan op away.

- Using scikits-cuda multiplication functions
    * adapt the batched_dot approach so the dot product uses a complex-complex dot implemented in scikits-cuda (hopefully this uses gemm under the hood?)
    * or, adapt the scan approach so the elementwise products use scikits-cuda functions. Unfortunately they do not support broadcasting (afaik), so we need to find a workaround for that.



- figure out if we can do the complex elemwise mult more efficiently with scikits-cuda (or within theano). This is definitely the bottleneck.

- rescaling afterwards is also quite slow (I think Theano turns this into an elemwise div, even though I wrote it as a multiplication). Maybe this can be sped up as well.

- if we stick with the scan approach, maybe introduce a "batch size" to tune memory usage vs. number of scan iterations.
    * if the compute batch size is > 1, each scan iteration does multiple elemwise multiplies in parallel
    * afterwards we still need to sum across the compute batch size




- build a convop optimisation that replaces the convop with the fft-product-ifft sequence.




- previous attempt by Josh Bleecher Snyder using CuFFT directly: https://github.com/lumberlabs/fft_conv_op/blob/master/fft_conv_op.py
    * we can steal the optimisation code from this to use it as a basis.

- FFTs can probably be done 'in place' as well, this could be faster. To make this feasible in Theano, we'd need an FFTOp and an InPlaceFFTOp, where the latter gets swapped in by an optimisation whenever this is possible.
    * find out if scikits.cuda.fft.fft is destructive by default!




    
Three possible approaches:

- implement a convop that does FFT, product, IFFT all in one go - a drop in replacement Op for the conv op.

- implement an FFT op and a macro function that can be used as a drop in replacement for the conv op, which creates fft, product and ifft nodes.
    * this allows theano to optimize everything.
    * have to implement the gradient of the FFT though.
    * might not be optimal for the gradient of a convolution, it's possible that this can be expressed more easily in the original conv form.

- implement an op that does FFT (does not necessarily support grad). Implement an op that functions as a conv op, but does not actually have a perform method. Instead, add an optimisation that swaps out the conv op for FFT+product+IFFT at compile time
    * same advantage as the macro: it allows theano to optimize everything
    * we don't need to implement the gradient of the FFT now, we can just implement the gradient of the conv op, and also express it as a conv op.

- maybe we can just write an optimization that swaps the ConvOp for FFTGpuConv, like it currently swaps it for GpuConv, which is in turn a macro for GpuFFT+product+GpuIFFT. No need for gradients anywhere then!



2D batch FFT with scikit cuda:

    def test_batch_fft_float64_to_complex128_2d(self):
        x = np.asarray(np.random.rand(self.B, self.N, self.M), np.float64)
        xf = np.fft.rfftn(x, axes=(1,2))
        x_gpu = gpuarray.to_gpu(x)
        xf_gpu = gpuarray.empty((self.B, self.N, self.M/2+1), np.complex128)
        plan = fft.Plan([self.N, self.M], np.float64, np.complex128, batch=self.B)
        fft.fft(x_gpu, xf_gpu, plan)
        assert np.allclose(xf, xf_gpu.get(), atol=atol_float64)

    => will need to use float32 and complex64 obviously.




process

    input:          (b, ic, i0, i1)                         float32
    filters:        (oc, ic, f0, f1)                        float32

    * pad filters
        filters:    (oc, ic, i0, i1)                        float32 # same size as input

    * reshape for FFT
        input:      (b * ic, i0, i1)                        float32
        filters:    (oc * ic, i0, i1)                       float32

    * perform FFT
        input:      (b * ic, i0, i1//2 + 1)                 complex64
        filters:    (oc * ic, i0, i1//2 + 1)                complex64

    * reshape again to unfold ic dimension, and to separate b and oc
        input:      (b, 1, ic, i0, i1//2 + 1)               complex64
        filters:    (1, oc, ic, i0, i1//2 + 1)              complex64

    * elementwise product
        output:     (b, oc, ic, i0, i1//2 + 1)              complex64

    * sum over the ic dimension (we can do this in the fourier domain because the FFT is linear)
        output:     (b, oc, i0, i1//2 + 1)                  complex64

    * reshape for IFFT
        output:     (b * oc, i0, i1//2 + 1)                 complex64

    * perform IFFT
        output:     (b * oc, i0, i1)                        float32

    * slice because the convolution was circular
        output:     (b * oc, i0 - f0 + 1, i1 - f1 + 1)      float32

    * reshape
        output:     (b, oc, i0 - f0 + 1, i1 - f1 + 1)       float32

        => result is a batched valid convolution of the input with the filters.




